{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (uncomment if needed)\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install wandb\n",
    "# !pip install ipywidgets\n",
    "# !pip install opencv-python\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import shufflenet_v2_x1_0\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Import Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Import ipywidgets for UI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Clear cache and collect garbage\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"/kaggle/input/ucf101\"\n",
    "\n",
    "class VideoClassificationConfig:\n",
    "    \"\"\"Enhanced configuration for video classification with I3D and ShuffleNet.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs: int = 50,\n",
    "        batch_size: int = 16,\n",
    "        learning_rate: float = 0.0001,\n",
    "        num_workers: int = 4,\n",
    "        videos_per_class: int = 10,\n",
    "        model_type: str = 'i3dshufflenet',  # 'i3d', 'shufflenet', or 'i3dshufflenet'\n",
    "        pretrained: bool = True,\n",
    "        accumulation_steps: int = 2,  # For gradient accumulation\n",
    "        use_amp: bool = True,  # Use Automatic Mixed Precision\n",
    "        max_batch_size: int = 32,  # Maximum batch size to attempt\n",
    "        wandb_project: str = 'har-i3dshufflenet-ucf101',\n",
    "        checkpoint_path: str = 'best_model_i3dshufflenet.pth',\n",
    "        resume: bool = True,  # Flag to resume training from checkpoint\n",
    "        scheduler_mode: str = 'plateau',  # 'plateau' or 'cosine'\n",
    "        scheduler_factor: float = 0.1,  # Factor for ReduceLROnPlateau\n",
    "        scheduler_patience: int = 5,  # Patience for ReduceLROnPlateau\n",
    "        early_stop_patience: int = 10,  # Patience for Early Stopping\n",
    "        checkpoint_interval: int = 10,  # Save checkpoint every N epochs\n",
    "        temporal_module: str = 'transformer',  # 'i3d', 'shuffle', or 'transformer'\n",
    "        use_attention: bool = True,\n",
    "        aux_loss: bool = True,\n",
    "        transformer_layers: int = 2,\n",
    "        transformer_heads: int = 4\n",
    "    ):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_workers = num_workers\n",
    "        self.videos_per_class = videos_per_class\n",
    "        self.model_type = model_type\n",
    "        self.pretrained = pretrained\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.use_amp = use_amp\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.wandb_project = wandb_project\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.resume = resume\n",
    "\n",
    "        # Scheduler parameters\n",
    "        self.scheduler_mode = scheduler_mode\n",
    "        self.scheduler_factor = scheduler_factor\n",
    "        self.scheduler_patience = scheduler_patience\n",
    "        \n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "\n",
    "        # Temporal module\n",
    "        self.temporal_module = temporal_module\n",
    "        self.transformer_layers = transformer_layers\n",
    "        self.transformer_heads = transformer_heads\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        # Auxiliary loss\n",
    "        self.aux_loss = aux_loss\n",
    "\n",
    "        # Device configuration\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Predefined action classes\n",
    "        self.classes = [\n",
    "            \"ApplyEyeMakeup\", \"ApplyLipstick\", \"Archery\", \"BabyCrawling\", \"BalanceBeam\",\n",
    "            \"BandMarching\", \"BaseballPitch\", \"Basketball\", \"BasketballDunk\", \"BenchPress\",\n",
    "            \"Biking\", \"Billiards\", \"BlowDryHair\", \"BlowingCandles\", \"BodyWeightSquats\",\n",
    "            \"Bowling\", \"BoxingPunchingBag\", \"BoxingSpeedBag\", \"BreastStroke\", \"BrushingTeeth\",\n",
    "            \"CleanAndJerk\", \"CliffDiving\", \"CricketBowling\", \"CricketShot\", \"CuttingInKitchen\",\n",
    "            \"Diving\", \"Drumming\", \"Fencing\", \"FieldHockeyPenalty\", \"FloorGymnastics\",\n",
    "            \"FrisbeeCatch\", \"FrontCrawl\", \"GolfSwing\", \"Haircut\", \"HammerThrow\",\n",
    "            \"Hammering\", \"HandstandPushups\", \"HandstandWalking\", \"HeadMassage\", \"HighJump\",\n",
    "            \"HorseRace\", \"HorseRiding\", \"HulaHoop\", \"IceDancing\", \"JavelinThrow\",\n",
    "            \"JugglingBalls\", \"JumpingJack\", \"JumpRope\", \"Kayaking\", \"Knitting\",\n",
    "            \"LongJump\", \"Lunges\", \"MilitaryParade\", \"Mixing\", \"MoppingFloor\",\n",
    "            \"Nunchucks\", \"ParallelBars\", \"PizzaTossing\", \"PlayingCello\", \"PlayingDaf\",\n",
    "            \"PlayingDhol\", \"PlayingFlute\", \"PlayingGuitar\", \"PlayingPiano\", \"PlayingSitar\",\n",
    "            \"PlayingTabla\", \"PlayingViolin\", \"PoleVault\", \"PommelHorse\", \"PullUps\",\n",
    "            \"Punch\", \"PushUps\", \"Rafting\", \"RockClimbingIndoor\", \"RopeClimbing\",\n",
    "            \"Rowing\", \"SalsaSpin\", \"ShavingBeard\", \"Shotput\", \"SkateBoarding\",\n",
    "            \"Skiing\", \"Skijet\", \"SkyDiving\", \"SoccerJuggling\", \"SoccerPenalty\",\n",
    "            \"StillRings\", \"SumoWrestling\", \"Surfing\", \"Swing\", \"TableTennisShot\",\n",
    "            \"TaiChi\", \"TennisSwing\", \"ThrowDiscus\", \"TrampolineJumping\", \"Typing\",\n",
    "            \"UnevenBars\", \"VolleyballSpiking\", \"WalkingWithDog\", \"WallPushups\", \"WritingOnBoard\",\n",
    "            \"YoYo\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "    \"\"\"Format frames to tensor with specified size.\"\"\"\n",
    "    frame = cv2.resize(frame, output_size)\n",
    "    frame = frame / 255.0  # Normalize to [0,1]\n",
    "    return frame\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames=32, output_size=(224, 224), frame_step=15):\n",
    "    \"\"\"Extract frames from video file.\"\"\"\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "    video_length = int(src.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "    if need_length > video_length:\n",
    "        start = 0\n",
    "    else:\n",
    "        max_start = video_length - need_length\n",
    "        start = random.randint(0, max_start + 1)\n",
    "\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "    ret, frame = src.read()\n",
    "\n",
    "    if not ret:\n",
    "        return np.zeros((n_frames, output_size[1], output_size[0], 3))\n",
    "\n",
    "    result.append(format_frames(frame, output_size))\n",
    "\n",
    "    for _ in range(n_frames - 1):\n",
    "        for _ in range(frame_step):\n",
    "            ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = format_frames(frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            # Pad with zeros if no more frames\n",
    "            result.append(np.zeros_like(result[0]))\n",
    "\n",
    "    src.release()\n",
    "\n",
    "    # Ensure exactly n_frames are returned\n",
    "    result = result[:n_frames]\n",
    "    while len(result) < n_frames:\n",
    "        result.append(np.zeros_like(result[0]))\n",
    "\n",
    "    result = np.array(result)\n",
    "    return result\n",
    "\n",
    "def adjust_batch_size(config, exception):\n",
    "    \"\"\"Adjust batch size dynamically in case of OOM.\"\"\"\n",
    "    if isinstance(exception, RuntimeError) and 'out of memory' in str(exception).lower():\n",
    "        if config.batch_size > 1:\n",
    "            config.batch_size = max(1, config.batch_size // 2)\n",
    "            logging.warning(f\"OOM detected. Reducing batch size to {config.batch_size}.\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Enhanced video dataset with improved frame sampling and preprocessing.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_paths: List[str],\n",
    "        targets: List[int],\n",
    "        config: VideoClassificationConfig,\n",
    "        n_frames: int = 32,\n",
    "        input_size: Tuple[int, int] = (224, 224)\n",
    "    ):\n",
    "        self.file_paths = file_paths\n",
    "        self.targets = targets\n",
    "        self.n_frames = n_frames\n",
    "        self.input_size = input_size\n",
    "        self.config = config\n",
    "\n",
    "        # Data augmentation transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.2\n",
    "            ),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        try:\n",
    "            # Extract and process frames\n",
    "            video_frames = self._extract_frames(self.file_paths[idx])\n",
    "\n",
    "            # Apply transforms frame by frame to reduce memory usage\n",
    "            transformed_frames = np.array([self.transform(frame) for frame in video_frames])\n",
    "\n",
    "            # Convert to tensor\n",
    "            video_tensor = torch.from_numpy(transformed_frames).float()\n",
    "\n",
    "            # **Correct Permutation**: [n_frames, C, H, W] -> [C, T, H, W]\n",
    "            video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "\n",
    "            label = self.targets[idx]\n",
    "\n",
    "            return video_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing video {self.file_paths[idx]}: {e}\")\n",
    "            # Return dummy data to prevent breaking the DataLoader\n",
    "            dummy_frames = torch.zeros(3, self.n_frames, *self.input_size)\n",
    "            return dummy_frames, torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "    def _extract_frames(self, video_path: str) -> np.ndarray:\n",
    "        return frames_from_video_file(\n",
    "            video_path,\n",
    "            n_frames=self.n_frames,\n",
    "            output_size=self.input_size\n",
    "        )\n",
    "\n",
    "def prepare_dataset(config: VideoClassificationConfig):\n",
    "    \"\"\"Prepare dataset by collecting video file paths and labels.\"\"\"\n",
    "    file_paths = []\n",
    "    targets = []\n",
    "\n",
    "    # **Update the dataset path accordingly**\n",
    "    dataset_root = dataset_root\n",
    "\n",
    "    for i, cls in enumerate(config.classes):\n",
    "        # Corrected glob pattern with recursive search\n",
    "        search_pattern = os.path.join(dataset_root, \"UCF101\", \"UCF-101\", cls, \"**\", \"*.avi\")\n",
    "        sub_file_paths = glob.glob(search_pattern, recursive=True)[:config.videos_per_class]\n",
    "        \n",
    "        if not sub_file_paths:\n",
    "            logging.warning(f\"No .avi files found for class '{cls}' in '{search_pattern}'.\")\n",
    "\n",
    "        file_paths += sub_file_paths\n",
    "        targets += [i] * len(sub_file_paths)\n",
    "\n",
    "    # Check if any video files were found\n",
    "    if not file_paths:\n",
    "        raise ValueError(\"No video files found. Please check the dataset path and file extensions.\")\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    combined = list(zip(file_paths, targets))\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    if combined:\n",
    "        file_paths, targets = zip(*combined)\n",
    "        file_paths = list(file_paths)\n",
    "        targets = list(targets)\n",
    "    else:\n",
    "        raise ValueError(\"No data found after shuffling.\")\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_paths, val_paths, train_targets, val_targets = train_test_split(\n",
    "        file_paths, targets, test_size=0.2, random_state=42, stratify=targets\n",
    "    )\n",
    "\n",
    "    return train_paths, val_paths, train_targets, val_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Block.\"\"\"\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channel, channel // reduction, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(channel // reduction, channel, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, t, h, w = x.size()\n",
    "        # Squeeze: Global Average Pooling\n",
    "        y = F.adaptive_avg_pool3d(x, 1).view(b, c)\n",
    "        # Excitation: FC -> ReLU -> FC -> Sigmoid\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(b, c, 1, 1, 1)\n",
    "        # Scale: Channel-wise multiplication\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Temporal Attention Module.\"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial Attention Module.\"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(channels, 1, kernel_size=7, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn = self.sigmoid(self.conv(x))  # [B, 1, T, H, W]\n",
    "        return x * attn\n",
    "\n",
    "class HybridAttention(nn.Module):\n",
    "    \"\"\"Hybrid Attention combining Temporal and Spatial Attention.\"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        self.temporal_attn = TemporalAttention(channels, reduction)\n",
    "        self.spatial_attn = SpatialAttention(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.temporal_attn(x)\n",
    "        x = self.spatial_attn(x)\n",
    "        return x\n",
    "\n",
    "class TransformerTemporalEncoder(nn.Module):\n",
    "    \"\"\"Transformer-based Temporal Encoder.\"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, num_layers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, T, C]\n",
    "        x = self.transformer_encoder(x)  # [B, T, C]\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class ChannelShuffle(nn.Module):\n",
    "    def __init__(self, groups: int):\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, time, height, width = x.size()\n",
    "        channels_per_group = channels // self.groups\n",
    "        x = x.view(batch, self.groups, channels_per_group, time, height, width)\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        x = x.view(batch, channels, time, height, width)\n",
    "        return x\n",
    "\n",
    "class TemporalShuffleBlock(nn.Module):\n",
    "    def __init__(self, channels: int, temporal_stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.temporal_conv = nn.Conv3d(\n",
    "            channels, channels, \n",
    "            kernel_size=(3, 1, 1),\n",
    "            stride=(temporal_stride, 1, 1),\n",
    "            padding=(1, 0, 0),\n",
    "            groups=channels\n",
    "        )\n",
    "        self.bn = nn.GroupNorm(num_groups=32, num_channels=channels)\n",
    "        self.shuffle = ChannelShuffle(groups=4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.temporal_conv(x)))\n",
    "        x = self.shuffle(x)\n",
    "        return x\n",
    "\n",
    "class AttentionBasedFusion(nn.Module):\n",
    "    \"\"\"Attention-Based Feature Fusion Module.\"\"\"\n",
    "    def __init__(self, i3d_channels: int, shufflenet_channels: int):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(i3d_channels, i3d_channels)\n",
    "        self.key = nn.Linear(shufflenet_channels, shufflenet_channels)\n",
    "        self.value = nn.Linear(shufflenet_channels, i3d_channels)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.fc = nn.Linear(i3d_channels + shufflenet_channels, i3d_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, i3d_feat, shufflenet_feat):\n",
    "        # i3d_feat: [B, C_i3d, T, H, W]\n",
    "        # shufflenet_feat: [B, C_shuff, T, H, W]\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        i3d_pooled = F.adaptive_avg_pool3d(i3d_feat, 1).view(i3d_feat.size(0), -1)  # [B, C_i3d]\n",
    "        shuff_pooled = F.adaptive_avg_pool3d(shufflenet_feat, 1).view(shufflenet_feat.size(0), -1)  # [B, C_shuff]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        Q = self.query(i3d_pooled)  # [B, C_i3d]\n",
    "        K = self.key(shuff_pooled)   # [B, C_shuff]\n",
    "        scores = torch.matmul(Q, K.transpose(0, 1)) / math.sqrt(Q.size(-1))  # [B, B]\n",
    "        attn_weights = self.softmax(scores)  # [B, B]\n",
    "        \n",
    "        # Apply attention\n",
    "        V = self.value(shuff_pooled)  # [B, C_i3d]\n",
    "        attn_output = torch.matmul(attn_weights, V)  # [B, C_i3d]\n",
    "        \n",
    "        # Fuse features\n",
    "        fused = torch.cat((i3d_pooled, attn_output), dim=1)  # [B, C_i3d + C_i3d]\n",
    "        fused = self.relu(self.fc(fused))  # [B, C_i3d]\n",
    "        \n",
    "        # Reshape to [B, C_i3d, 1, 1, 1] and expand\n",
    "        fused = fused.view(fused.size(0), fused.size(1), 1, 1, 1)\n",
    "        fused = fused.expand_as(i3d_feat)  # [B, C_i3d, T, H, W]\n",
    "        \n",
    "        # Add residual connection\n",
    "        fused = fused + i3d_feat  # [B, C_i3d, T, H, W]\n",
    "        \n",
    "        return fused\n",
    "\n",
    "class EnhancedI3DShuffleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced hybrid architecture combining I3D and ShuffleNet concepts\n",
    "    with advanced features for video classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        pretrained: bool = True,\n",
    "        dropout_prob: float = 0.5,\n",
    "        temporal_module: str = 'transformer',  # 'i3d', 'shuffle', or 'transformer'\n",
    "        use_attention: bool = True,\n",
    "        aux_loss: bool = False,\n",
    "        transformer_layers: int = 2,\n",
    "        transformer_heads: int = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # I3D backbone initialization\n",
    "        if pretrained:\n",
    "            weights = R3D_18_Weights.DEFAULT\n",
    "            self.i3d_backbone = r3d_18(weights=weights)\n",
    "        else:\n",
    "            self.i3d_backbone = r3d_18(weights=None)\n",
    "            \n",
    "        # ShuffleNet backbone initialization\n",
    "        self.shuffle_backbone = shufflenet_v2_x1_0(pretrained=pretrained)\n",
    "        \n",
    "        # Get feature dimensions\n",
    "        self.i3d_features = self.i3d_backbone.fc.in_features  # Typically 512 for r3d_18\n",
    "        self.shuffle_features = self.shuffle_backbone.fc.in_features  # Typically 1024 for shufflenet_v2_x1_0\n",
    "        \n",
    "        # Remove original fully connected layers\n",
    "        self.i3d_backbone.fc = nn.Identity()\n",
    "        self.shuffle_backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Temporal modeling\n",
    "        self.temporal_module = temporal_module\n",
    "        if temporal_module == 'shuffle':\n",
    "            self.temporal_blocks = nn.ModuleList([\n",
    "                TemporalShuffleBlock(self.i3d_features // 2),\n",
    "                TemporalShuffleBlock(self.i3d_features // 2)\n",
    "            ])\n",
    "        elif temporal_module == 'transformer':\n",
    "            self.temporal_encoder = TransformerTemporalEncoder(\n",
    "                embed_dim=self.i3d_features,\n",
    "                num_heads=transformer_heads,\n",
    "                num_layers=transformer_layers,\n",
    "                dropout=0.1\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported temporal_module: {temporal_module}\")\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            self.attention = HybridAttention(self.i3d_features)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = AttentionBasedFusion(self.i3d_features, self.shuffle_features)\n",
    "        \n",
    "        # Enhanced classifier head with residual connections and GroupNorm\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout3d(dropout_prob),\n",
    "            nn.Conv3d(self.i3d_features, self.i3d_features // 2, kernel_size=1),\n",
    "            nn.GroupNorm(num_groups=16, num_channels=self.i3d_features // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout3d(dropout_prob),\n",
    "            nn.Conv3d(self.i3d_features // 2, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        # Auxiliary classifier for deep supervision\n",
    "        self.aux_loss = aux_loss\n",
    "        if aux_loss:\n",
    "            self.aux_classifier = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool3d(1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.i3d_features, self.i3d_features // 2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout_prob),\n",
    "                nn.Linear(self.i3d_features // 2, num_classes)\n",
    "            )\n",
    "        \n",
    "    def _process_shuffle_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, t, h, w = x.shape\n",
    "        x = x.transpose(1, 2).contiguous()  # [B, T, C, H, W]\n",
    "        x = x.view(-1, c, h, w)  # [B*T, C, H, W]\n",
    "        x = self.shuffle_backbone(x)  # [B*T, C_shuff]\n",
    "        x = x.view(b, t, -1, 1, 1)  # [B, T, C_shuff, 1, 1]\n",
    "        x = x.transpose(1, 2).contiguous()  # [B, C_shuff, T, 1, 1]\n",
    "        return x.expand(-1, -1, t, h, w)  # [B, C_shuff, T, H, W]\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # I3D feature extraction\n",
    "        i3d_features = self.i3d_backbone(x)  # [B, C_i3d, T, H, W]\n",
    "        \n",
    "        # ShuffleNet feature extraction\n",
    "        shuffle_features = self._process_shuffle_features(x)  # [B, C_shuff, T, H, W]\n",
    "        \n",
    "        # Temporal modeling\n",
    "        if self.temporal_module == 'shuffle':\n",
    "            split_size = i3d_features.size(1) // 2\n",
    "            x1, x2 = torch.split(i3d_features, split_size, dim=1)\n",
    "            x1 = self.temporal_blocks[0](x1)\n",
    "            x2 = self.temporal_blocks[1](x2)\n",
    "            i3d_features = torch.cat([x1, x2], dim=1)\n",
    "        elif self.temporal_module == 'transformer':\n",
    "            b, c, t, h, w = i3d_features.size()\n",
    "            i3d_pooled = F.adaptive_avg_pool3d(i3d_features, (1, h, w)).view(b, c, t)  # [B, C, T]\n",
    "            i3d_pooled = i3d_pooled.permute(0, 2, 1)  # [B, T, C]\n",
    "            i3d_pooled = self.temporal_encoder(i3d_pooled)  # [B, T, C]\n",
    "            i3d_pooled = i3d_pooled.permute(0, 2, 1).unsqueeze(-1).unsqueeze(-1)  # [B, C, T, 1, 1]\n",
    "            i3d_features = i3d_features + i3d_pooled  # Residual connection\n",
    "        \n",
    "        # Apply attention if enabled\n",
    "        if self.use_attention:\n",
    "            i3d_features = self.attention(i3d_features)\n",
    "        \n",
    "        # Feature fusion using Attention-Based Fusion\n",
    "        fused_features = self.fusion(i3d_features, shuffle_features)  # [B, C_i3d, T, H, W]\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(fused_features)  # [B, num_classes, T, H, W]\n",
    "        output = F.adaptive_avg_pool3d(output, 1).view(output.size(0), -1)  # [B, num_classes]\n",
    "        \n",
    "        if self.aux_loss and self.training:\n",
    "            aux_output = self.aux_classifier(i3d_features)  # [B, num_classes]\n",
    "            return output, aux_output\n",
    "                \n",
    "        return output\n",
    "\n",
    "def create_model(config: VideoClassificationConfig) -> nn.Module:\n",
    "    \"\"\"Factory method to create appropriate video classification model.\"\"\"\n",
    "    num_classes = len(config.classes)\n",
    "\n",
    "    if config.model_type == 'i3dshufflenet':\n",
    "        model = EnhancedI3DShuffleNet(\n",
    "            num_classes=num_classes,\n",
    "            pretrained=config.pretrained,\n",
    "            dropout_prob=0.5,\n",
    "            temporal_module=config.temporal_module,  # 'i3d', 'shuffle', or 'transformer'\n",
    "            use_attention=config.use_attention,\n",
    "            aux_loss=config.aux_loss,\n",
    "            transformer_layers=config.transformer_layers,\n",
    "            transformer_heads=config.transformer_heads\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {config.model_type}\")\n",
    "\n",
    "    model = model.to(config.device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, scaler, config):\n",
    "    \"\"\"Train model for one epoch with enhanced logging and progress tracking.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = len(dataloader)\n",
    "\n",
    "    # Progress bar for the entire epoch\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=batches, desc=\"Training\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (videos, labels) in progress_bar:\n",
    "        videos, labels = videos.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=config.use_amp):\n",
    "            outputs = model(videos)\n",
    "            if config.aux_loss and isinstance(outputs, tuple):\n",
    "                main_output, aux_output = outputs\n",
    "                loss1 = criterion(main_output, labels)\n",
    "                loss2 = criterion(aux_output, labels)\n",
    "                loss = loss1 + 0.4 * loss2 \n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            loss = loss / config.accumulation_steps\n",
    "\n",
    "        # Gradient accumulation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (batch_idx + 1) % config.accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Compute batch statistics\n",
    "        running_loss += loss.item() * config.accumulation_steps\n",
    "        if config.aux_loss and isinstance(outputs, tuple):\n",
    "            _, predicted = main_output.max(1)\n",
    "        else:\n",
    "            _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Compute and update progress bar\n",
    "        current_loss = running_loss / (batch_idx + 1)\n",
    "        current_acc = 100. * correct / total\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{current_loss:.4f}',\n",
    "            'Accuracy': f'{current_acc:.2f}%'\n",
    "        })\n",
    "\n",
    "        # Log to Weights & Biases every batch\n",
    "        wandb.log({\n",
    "            'Train/Loss': current_loss,\n",
    "            'Train/Accuracy': current_acc,\n",
    "            'Train/Batch': batch_idx + 1\n",
    "        })\n",
    "\n",
    "    # Compute epoch-level metrics\n",
    "    epoch_loss = running_loss / batches\n",
    "    epoch_acc = 100. * correct / total\n",
    "\n",
    "    logging.info(f\"\\nTraining Epoch Summary:\")\n",
    "    logging.info(f\"Epoch Loss: {epoch_loss:.4f}\")\n",
    "    logging.info(f\"Epoch Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "    # Log epoch metrics to Weights & Biases\n",
    "    wandb.log({\n",
    "        'Train/Epoch Loss': epoch_loss,\n",
    "        'Train/Epoch Accuracy': epoch_acc\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device, config):\n",
    "    \"\"\"Validate model performance with enhanced logging and tracking.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batches = len(dataloader)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Progress bar for validation\n",
    "    progress_bar = tqdm(dataloader, total=batches, desc=\"Validation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (videos, labels) in enumerate(progress_bar):\n",
    "            videos, labels = videos.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=config.use_amp):\n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute batch statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Collect predictions for final analysis\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Compute and update progress bar\n",
    "            current_loss = running_loss / (batch_idx + 1)\n",
    "            current_acc = 100. * correct / total\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Accuracy': f'{current_acc:.2f}%'\n",
    "            })\n",
    "\n",
    "            # Log to Weights & Biases every batch\n",
    "            wandb.log({\n",
    "                'Validation/Loss': current_loss,\n",
    "                'Validation/Accuracy': current_acc,\n",
    "                'Validation/Batch': batch_idx + 1\n",
    "            })\n",
    "\n",
    "    # Compute validation-level metrics\n",
    "    val_loss = running_loss / batches\n",
    "    val_acc = 100. * correct / total\n",
    "\n",
    "    # Calculate additional metrics with zero_division=0\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=config.classes, zero_division=0)\n",
    "    logging.info(f\"\\nValidation Summary:\")\n",
    "    logging.info(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    logging.info(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "    logging.info(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Log metrics to Weights & Biases\n",
    "    wandb.log({\n",
    "        'Validation/Epoch Loss': val_loss,\n",
    "        'Validation/Epoch Accuracy': val_acc,\n",
    "        'Validation/Precision': precision,\n",
    "        'Validation/Recall': recall,\n",
    "        'Validation/F1-Score': f1\n",
    "    })\n",
    "\n",
    "    return val_loss, val_acc, all_preds, all_labels, class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(config, model, optimizer, scheduler, scaler):\n",
    "    \"\"\"Load checkpoint if available.\"\"\"\n",
    "    if os.path.isfile(config.checkpoint_path):\n",
    "        logging.info(f\"Loading checkpoint from '{config.checkpoint_path}'\")\n",
    "        checkpoint = torch.load(config.checkpoint_path, map_location=config.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_acc = checkpoint.get('best_val_acc', 0)\n",
    "        logging.info(f\"Loaded checkpoint '{config.checkpoint_path}' (Epoch {start_epoch})\")\n",
    "        return start_epoch, best_val_acc\n",
    "    else:\n",
    "        logging.info(f\"No checkpoint found at '{config.checkpoint_path}'. Starting fresh.\")\n",
    "        return 0, 0\n",
    "\n",
    "def get_current_lr(optimizer):\n",
    "    \"\"\"Retrieve the current learning rate from the optimizer.\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Accuracy subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Loss subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_custom(y_true, y_pred, class_names):\n",
    "    \"\"\"Generate and plot confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "\n",
    "config = VideoClassificationConfig(\n",
    "    model_type='i3dshufflenet',\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.0001,\n",
    "    accumulation_steps=2,\n",
    "    use_amp=True,\n",
    "    wandb_project='thanhnx',\n",
    "    checkpoint_path='best_model_i3dshufflenet.pth',\n",
    "    resume=True,\n",
    "    scheduler_mode='plateau',\n",
    "    scheduler_factor=0.1,\n",
    "    scheduler_patience=5,\n",
    "    early_stop_patience=10,\n",
    "    checkpoint_interval=10,\n",
    "    temporal_module='transformer',  # 'i3d', 'shuffle', or 'transformer'\n",
    "    use_attention=True,\n",
    "    aux_loss=True,\n",
    "    transformer_layers=2,\n",
    "    transformer_heads=4\n",
    ")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training_i3dshufflenet.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=config.wandb_project,\n",
    "    config=vars(config),\n",
    "    resume='allow' if config.resume else False,\n",
    "    job_type='training',\n",
    "    settings=wandb.Settings(init_timeout=120)\n",
    ")\n",
    "\n",
    "# Prepare dataset\n",
    "train_paths, val_paths, train_targets, val_targets = prepare_dataset(config)\n",
    "logging.info(f\"Training samples: {len(train_paths)}, Validation samples: {len(val_paths)}\")\n",
    "wandb.config.update({\n",
    "    'train_samples': len(train_paths),\n",
    "    'val_samples': len(val_paths)\n",
    "})\n",
    "\n",
    "train_dataset = VideoDataset(train_paths, train_targets, config)\n",
    "val_dataset = VideoDataset(val_paths, val_targets, config)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = create_model(config)\n",
    "logging.info(f\"Using model type: {config.model_type}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_targets),\n",
    "    y=train_targets\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(config.device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Initialize ReduceLROnPlateau scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',  # Since we are monitoring accuracy\n",
    "    factor=config.scheduler_factor,\n",
    "    patience=config.scheduler_patience,\n",
    "    verbose=True,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Initialize mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.use_amp)\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_acc = 0\n",
    "if config.resume:\n",
    "    start_epoch, best_val_acc = load_checkpoint(config, model, optimizer, scheduler, scaler)\n",
    "\n",
    "# Training loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(start_epoch, config.epochs):\n",
    "    if early_stop:\n",
    "        logging.info(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{config.epochs}')\n",
    "    logging.info(f'Epoch {epoch+1}/{config.epochs}')\n",
    "    wandb.log({'epoch': epoch + 1})\n",
    "\n",
    "    try:\n",
    "        # Training phase\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config.device, scaler, config\n",
    "        )\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc, all_preds, all_labels, class_report = validate(\n",
    "            model, val_loader, criterion, config.device, config\n",
    "        )\n",
    "\n",
    "        # Step the scheduler with validation accuracy\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        # Retrieve and log the current learning rate\n",
    "        current_lr = get_current_lr(optimizer)\n",
    "        wandb.log({'Learning Rate': current_lr})\n",
    "        logging.info(f\"Current Learning Rate: {current_lr}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "            }, config.checkpoint_path)\n",
    "            logging.info(f\"New best model saved with validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "            # Create a W&B Artifact\n",
    "            artifact = wandb.Artifact('best_model_i3dshufflenet', type='model')\n",
    "            artifact.add_file(config.checkpoint_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "            logging.info(f\"Checkpoint {config.checkpoint_path} logged to W&B Artifacts.\")\n",
    "            print(f\"Checkpoint {config.checkpoint_path} logged to W&B Artifacts.\")\n",
    "\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            logging.info(f\"No improvement in validation accuracy for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "        # Save checkpoint every N epochs\n",
    "        if (epoch + 1) % config.checkpoint_interval == 0:\n",
    "            checkpoint_name = f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "            }, checkpoint_name)\n",
    "            logging.info(f\"Checkpoint saved at epoch {epoch+1} as '{checkpoint_name}'.\")\n",
    "\n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        logging.info(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        logging.info(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve >= config.early_stop_patience:\n",
    "            logging.info(f\"No improvement for {config.early_stop_patience} consecutive epochs. Stopping training.\")\n",
    "            early_stop = True\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if adjust_batch_size(config, e):\n",
    "            # Reinitialize data loaders with new batch size\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=config.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=config.num_workers,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True\n",
    "            )\n",
    "\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=config.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=config.num_workers,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True\n",
    "            )\n",
    "\n",
    "            # Reinitialize optimizer and scheduler\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='max',\n",
    "                factor=config.scheduler_factor,\n",
    "                patience=config.scheduler_patience,\n",
    "                verbose=True,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "            scaler = torch.cuda.amp.GradScaler(enabled=config.use_amp)\n",
    "\n",
    "            # Retry the current epoch\n",
    "            epoch -= 1\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            continue\n",
    "        else:\n",
    "            logging.error(\"Unrecoverable CUDA OOM error.\")\n",
    "            raise e\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "wandb.finish()\n",
    "plot_training_history(history)\n",
    "plot_confusion_matrix_custom(all_labels, all_preds, config.classes)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "final_model_path = 'final_model_i3dshufflenet.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "logging.info(f\"Final model saved to '{final_model_path}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thanhnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
